{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Bidirectional, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import Adam, AdamW\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, BinaryCrossentropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import lightgbm as lgb\n",
    "import shap\n",
    "import optuna\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(r\"C:\\Users\\LENOVO FLEX\\Downloads\\Intrusion Detection System\\UNSW_NB15_training-set.csv\")\n",
    "\n",
    "# Drop ID column if present\n",
    "if 'id' in data.columns:\n",
    "    data = data.drop(columns=['id'])\n",
    "\n",
    "# Encode categorical features\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    data[col] = LabelEncoder().fit_transform(data[col])\n",
    "\n",
    "# Separate features and labels\n",
    "X = data.drop(columns=['attack_cat', 'label'])\n",
    "y_multi = data['attack_cat']  # Multiclass labels\n",
    "y_binary = data['label']  # Binary labels\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Feature Selection using LightGBM + SHAP\n",
    "lgb_model = lgb.LGBMClassifier()\n",
    "lgb_model.fit(X_scaled, y_binary)\n",
    "explainer = shap.Explainer(lgb_model)\n",
    "shap_values = explainer(X_scaled)\n",
    "\n",
    "# Get top 15 important features\n",
    "important_features = np.argsort(np.abs(shap_values.values).mean(axis=0))[-15:]  # Top 15 features\n",
    "X_selected = X_scaled[:, important_features]\n",
    "\n",
    "# Print the selected features\n",
    "feature_names = X.columns[important_features]\n",
    "print(\"Selected Features after LightGBM + SHAP:\")\n",
    "print(feature_names)\n",
    "\n",
    "# Train-test split for binary and multiclass\n",
    "X_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(X_selected, y_binary, test_size=0.2, stratify=y_binary)\n",
    "X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(X_selected, y_multi, test_size=0.2, stratify=y_multi)\n",
    "\n",
    "# Handle class imbalance using SMOTE\n",
    "smote = SMOTE()\n",
    "X_train_bin, y_train_bin = smote.fit_resample(X_train_bin, y_train_bin)\n",
    "X_train_multi, y_train_multi = smote.fit_resample(X_train_multi, y_train_multi)\n",
    "\n",
    "# Reshape for LSTM input (for multiclass model)\n",
    "X_train_multi_lstm = np.expand_dims(X_train_multi, axis=1)\n",
    "X_test_multi_lstm = np.expand_dims(X_test_multi, axis=1)\n",
    "\n",
    "# Compute class weights for multiclass model\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_multi), y=y_train_multi)\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "# Multiclass Model (LSTM with Attention)\n",
    "def create_multiclass_model(trial):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_train_multi_lstm.shape[1], X_train_multi_lstm.shape[2])))\n",
    "\n",
    "    # First Bidirectional LSTM layer\n",
    "    model.add(Bidirectional(LSTM(units=trial.suggest_int(\"units_1\", 64, 256, step=32), return_sequences=True)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(trial.suggest_float(\"dropout_1\", 0.2, 0.5)))\n",
    "\n",
    "    # Second Bidirectional LSTM layer\n",
    "    model.add(Bidirectional(LSTM(units=trial.suggest_int(\"units_2\", 32, 128, step=16))))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(trial.suggest_float(\"dropout_2\", 0.2, 0.5)))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(len(np.unique(y_train_multi)), activation='softmax'))\n",
    "\n",
    "    # Compile model\n",
    "    optimizer = Adam(learning_rate=trial.suggest_loguniform(\"lr\", 1e-4, 1e-2))\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Hyperparameter Optimization for Multiclass Model\n",
    "def objective(trial):\n",
    "    model = create_multiclass_model(trial)\n",
    "    history = model.fit(X_train_multi_lstm, y_train_multi, validation_data=(X_test_multi_lstm, y_test_multi),\n",
    "                        epochs=12, batch_size=trial.suggest_categorical(\"batch_size\", [32, 64, 128]),\n",
    "                        class_weight=class_weights_dict, verbose=0,\n",
    "                        callbacks=[EarlyStopping(monitor='val_loss', patience=3)])\n",
    "    return max(history.history['val_accuracy'])\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Train Best Multiclass Model\n",
    "best_params = study.best_params\n",
    "multiclass_model = create_multiclass_model(optuna.trial.FixedTrial(best_params))\n",
    "multiclass_model.fit(X_train_multi_lstm, y_train_multi, validation_data=(X_test_multi_lstm, y_test_multi),\n",
    "                     epochs=50, batch_size=best_params['batch_size'], class_weight=class_weights_dict,\n",
    "                     callbacks=[EarlyStopping(monitor='val_loss', patience=5)])\n",
    "\n",
    "# Evaluate Multiclass Model\n",
    "multiclass_accuracy = multiclass_model.evaluate(X_test_multi_lstm, y_test_multi)[1] * 100\n",
    "print(f'Multiclass Model Accuracy: {multiclass_accuracy:.2f}%')\n",
    "\n",
    "# Save Multiclass Model\n",
    "multiclass_model.save('/content/sample_data/multiclass_model.h5')\n",
    "print(\"Multiclass Model saved successfully.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Binary Classification Model (DNN)\n",
    "binary_model = Sequential([\n",
    "    Dense(128, activation='selu', input_shape=(X_train_bin.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='selu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "binary_model.compile(optimizer=AdamW(), loss=BinaryCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "# Train Binary Model\n",
    "binary_model.fit(X_train_bin, y_train_bin, validation_data=(X_test_bin, y_test_bin), epochs=20, batch_size=32)\n",
    "\n",
    "# Evaluate Binary Model\n",
    "binary_loss, binary_acc = binary_model.evaluate(X_test_bin, y_test_bin)\n",
    "print(f'Binary Model Accuracy: {binary_acc * 100:.2f}%')\n",
    "\n",
    "# Save Binary Model\n",
    "binary_model.save('/content/sample_data/binary_model.h5')\n",
    "print(\"Binary Model saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
